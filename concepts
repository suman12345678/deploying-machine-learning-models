skleran pipeline has transformer, predictor and pipeline

PATH and PYTHONPATH
===================
1. Update PATH where python installed in environment variable,
  after this you can type python and go into python terminal (doesnot matter where you present working directory in command prompt)
  if this is not done message 'python is not recognize as internal or external command'

  if python wants to look for module and packages from a project, update PYTHONPATH with that foldername

VIRTUALENV
==========
create virtual environment to maintain different version of package for different project. DO the below in command prompt or anaconde prompt
1. python -m venv demo_proj [venv is in standard library]
the above will create a folder in current dir, we can install pkg there only for that environment
2. demo_proj\Scripts\activate will activate
3. pip install numpy
4. deactive

GITHUB
======
1. install git and login to github
2. git config --global user.name suman12345678
3. git config --global user.email suman.ju06@gmail.com
4. git clone https://github.com/suman12345678/deploying-machine-learning-models.git ==> will create a local copy in current directory
5. to checkout any specifi commit: 
   login to the folder (deploying-machine-learning-models)
   git checkout  8d79cf4212c7f736a69b6c8d0128ef77232374ea
   so this will take the folder to initial some stage
6. git remote set-url origin https://github.com/suman12345678/deploying-machine-learning-models.git
pull request(set desktop with git url map before pushing)
a. git checkout -b test-branch-2
b. git commit --allow-empty -m "pull req"
c. git push origin test-branch-2

SETUP ML FRAMEWORK
==================
1. there is one setup.py we need to run that by below(Building the package, make sure you cd to root dir)
run command: python packages\regression_model\setup.py sdist bdist_wheel

2. we can install local packages : pip install -e packages\regression_model  (this will make the model importable)

Once both steps are done you can do: import regression_model;regression_model.__version__


MANIFEST.in file specify which directory we wish to include in our packages

********************************************************************************************************
********************************************************************************************************
Custom pipeline
===============
Here we will create 2 classes A) customPipeLineTrain.ipynb and B) customPipelineProcessors.ipynb
A)
customPipeLineTrain.ipynb
-------------------------
import pandas as pd
import numpy as np
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler

from regression_model.processing.processing_pipeline import FeaturePreparer


class ModelManager:
    def __init__(self, *,
                 feature_preparer: FeaturePreparer,
                 scaler: StandardScaler):
        self.processor = feature_preparer
        self.scaler = scaler
        self.trained_model = None # will save once trained


    def get_training_vars(self):
        return [
            var for var in self.processor.X_train.columns
            if var not in ['Id', 'SalePrice']
        ]

    def prepare_scaler(self):
        training_vars = self.get_training_vars()
        # fit the scaler to the train set for later use
        self.scaler.fit(self.processor.X_train[training_vars])


    def fit_model(self, *, training_vars: t.List[str]):
        lin_model = Lasso(random_state=2908)
        lin_model.fit(self.scaler.transform(
            self.processor.X_train[training_vars]),
            self.processor.y_train
        )
        self.trained_model = lin_model



    def run_pipeline(self, *, training: bool=False) -> None:
        """
        Performs the data preparation steps.
        The sequence of these steps is important.
        The pipeline will output a prepared dataset.
        The pipeline will output the scored dataset.
        """
        # trains the processor if train=True
        self.processor.prepare_data(training=training)
        self.prepare_scaler()
        training_vars = self.get_training_vars()
        
        if training:
            self.fit_model(training_vars=training_vars)
        
        pred = self.trained_model.predict(
            self.scaler.transform(self.processor.prepared_data[training_vars]))

        return pred


if __name__ == '__main__':
    raw_data = load_dataset(file_name='house_price_train.csv')
    processor = FeaturePreparer(raw_data=raw_data)
    scaler = StandardScaler()
    manager = ModelManager(feature_preparer=processor,
                           scaler=scaler)
    manager.run_pipeline(training=True)
    
    
 B) customPipelineProcessors.ipynb
 In [ ]:
"""
List of feature engineering steps
=================================
This script is intended as an example of how to write a custom pipeline
and does not reproduce entirely the engineering procedures we described in 
section 2.

This pipeline is not intended to be run as is.
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# feature scaling
from sklearn.preprocessing import StandardScaler


class FeaturePreparer:
    # when we call the Feature preparer for the first time
    # we initialise it with the training data set, which is 
    # then stored as an attribute(raw_data)
    def __init__(self, raw_data: pd.DataFrame):
        self.raw_data = raw_data
        self.prepared_data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.continuous = None
        self.categorical = None
        self.discrete = None
        self.encoding_dict = {}

    
    def separate_variable_types(self) -> None:
        # find categorical variables
        # this should be done based off training data, i.e., self.raw_data
        self.categorical = [
            var for var in self.raw_data.columns
            if self.war_data[var].dtype == 'O'
        ]
        print('There are {} categorical variables'.format(len(self.categorical)))

        # find numerical variables
        # this should be done based off training data, i.e., self.raw_data
        numerical = [var for var in self.raw_data.columns
                     if self.raw_data[var].dtype != 'O']
        print('There are {} numerical variables'.format(len(numerical)))

        # find discrete variables
        # this should be done based off training data, i.e., self.raw_data
        self.discrete = []
        for var in numerical:
            if len(self.raw_data[var].unique()) < 20:
                self.discrete.append(var)

        print('There are {} discrete variables'.format(len(self.discrete)))

        self.continuous = [
            var for var in numerical if
            var not in self.discrete and var not in ['Id', 'SalePrice']
        ]

    def split_data(self, *, training: bool = False) -> None:
        # if we are training for the first time, training = True, 
        # then divide into train and test
        if training:
            return
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.prepared_data, self.prepared_data.SalePrice, test_size=0.2, random_state=0)

        print(self.X_train.shape, self.X_test.shape)


    def handle_missing_values(self):
        # fills NA in all required variables
        for col in self.continuous:
            if self.prepared_data.loc[:, (col)].isnull().mean() > 0:
                # get the mean value from the training data, i.e., self.raw_data
                mean_val = self.raw_data.loc[:, (col)].mean()
                # replace it in the data to be passed to the model, i.e., self.prepared_data
                self.prepared_data[col].fillna(mean_val, inplace=True)

        # add label indicating 'Missing' to categorical variables
        for var in self.categorical:
            # replace NA in all categorical variables to be passed to the model
            self.prepared_data[var].fillna('Missing', inplace=True)


    def rare_imputation(self, *, variable):
        # find frequent labels / discrete numbers in training data
        temp = self.raw_data.groupby([variable])[variable].count() / np.float(len(self.raw_data))
        frequent_cat = [x for x in temp.loc[temp > 0.03].index.values]

        # replace the labels in data to be passed to model
        self.prepared_data[variable] = np.where(self.prepared_data[variable].isin(frequent_cat),
                                          self.prepared_data[variable],
                                     'Rare')


    def encode_categorical_variables(self, *, var, target, training: bool = False) -> None:
        if training:
        # make label to price dictionary
            self.encoding_dict[var] = self.prepared_data.groupby([var])[target].mean().to_dict()

        # encode variables
        self.prepared_data[var] = self.prepared_data[var].map(self.encoding_dict[var])


    def prepare_data(self, training: bool = False):
        # This is the method where we capture  all the parameters during training,
        # from the training set, 
        # then we will use this parameters to transform future data as we get it
        # from our API

        # if training = true, the function will store the transformed
        # data
        # if training = false, the function will only transform the passed
        # data.
        # This way we can use the class both for training and scoring, and
        # we will have the data stored for future use
        self.prepared_data = self.raw_data.copy(deep=True)

        self.separate_data_types() # captures the different types of vars
        self.handle_missing_values() # fills na

        for var in self.categorical + self.discrete:
            self.rare_imputation(variable=var) # replaces rare labels
            
            self.encode_categorical_variables(
                var=var,
                target='SalePrice',
                training=training) # encode labels in categorical vars
                                   # will store the encoding dict if training = True

        self.split_data(training=training) # splits data only for training
        
        if not training:
            if 'SalePrice' in self.prepared_data.columns:
                self.prepared_data.drop(['SalePrice'], axis=1)

********************************************************************************************************
********************************************************************************************************
FLASK
=====


creating flask
==============

run by
======

set/export FLASK_APP=hello.py    [environment variable; where __name__=="__main__" exist]
python3 hello.py/ flask run [to start program at 0.0.0.0:5000 ]



project structure we will seperate some code from above as all are written in hello.py. rename hello.py to routes.py. 
=================
sampleproject/

   projenv/

      ....

   helloapp/

       templates/

          index.html  {% if user %} {% else %} {% endif %}; {% for user in users %} {% endfor %} ; can be added for conditional
	  ==========
		<html>
    		<head>
       		 <title>{{ title }}</title>
   		</head>
    		<body>
        		<div>HelloApp: <a href="/">Home</a></div>
        		<hr>
			{% block content %}{% endblock %}   # this can be used by other html to inherit index.html
        		<h1>Hello World!!!</h1>
    		</body>
		</html>

          inherited.html 
	  ==============
		{% extends 'index.html' %}
		{% block content %}
  		      {% if user %}
   		     <h1>Hello {{user}}!!!</h1>
   		     {% else %}
   		     <h1>Hello World!!!</h1>
    		    {% endif %}
		{% endblock %}


	  adduser.html  (for webform)
	  ============
		{% extends 'base.html' %}
		{% block content %}
		    <h2 style = "text-align: center;">User Input Form</h2>
		<form action = "" method = post>
		         <fieldset>
		            <legend>User Input Form</legend>
		            {{ form.csrf_token }}
		            <div style = "font-size:20px; font-weight:bold; margin-left:150px;" >
		               {{ form.fname.label }} : {{ form.fname }} <br>
				{% for error in form.fname.errors %}
				<span style="color: red;">[{{ error }}]</span><br>
				{% endfor %}
			       <br>
		               {{ form.lname.label }} : {{ form.lname }} <br>
				{% for error in form.lname.errors %}
				<span style="color: red;">[{{ error }}]</span><br>
				{% endfor %}
			       <br>
		               {{ form.email.label }} : {{ form.email }} <br>
				{% for error in form.email.errors %}
				<span style="color: red;">[{{ error }}]</span><br>
				{% endfor %}
			       <br>
		               {{ form.submit }}
		            </div>
		         </fieldset>
		      </form>
		{% endblock %}

		

       __init__.py    : 
       ===========	
		from flask import Flask;
		from .config import Config;   
		from flask_sqlalchemy import SQLAlchemy;  [for database]
		from flask_migrate import Migrate  [propagate the model class definition into the database schema]
		app = Flask(__name__); 
		app.config.from_object(Config)
		db = SQLAlchemy(app)
		migrate = Migrate(app, db)
		from helloapp import routes

       routes.py
       =========
		from flask import Flask [will be remove to other location  __init__  of projfolder later]
		from flask import redirect, url_for
		from flask import render_template
		from .forms import UserForm   [forms.py and routes.py are in same level fo put .forms here]
		from flask import request

		@app.route("/adduser/")
		def useradd():
   		form = UserForm()
   		return render_template('adduser.html', title = 'User Input Form', form = form)

		@app.route("/")
		def hello():
 		  return "Hello World!!!"

		@app.route("/user/<username>/<int:age>/")
		def display_age(username, age):
		    return "Hello " + username +"!!!<br>You are " + str(age) + " years old."

		@app.route("/greet/user/<uname>")
		def greet_user(uname):
  		 return redirect(url_for('hello_user', username=uname))

		@app.route("/user/<username>/")
		def hello_user(username):
 		   return '''
		<html>
 		   <head>
 		      <title>User Page</title>
 		   </head>
 		   <body>
  		      <h1>Hello, ''' + username + '''!!!</h1>
  		  </body>
		</html>'''

		#Lets create a template index.html(inside templates ) of html for better structure. let's create a folder named templates in helloapp folder.
		@app.route("/")
		def hello():
  		  return render_template("index.html", title="Title Page of Hello App")

		@app.route("/adduser/", methods=['GET', 'POST'])  # receive form data from user
		def useradd():
  		 form = UserForm()
  		 if request.method == 'POST':
	
		      if form.validate() == False:
             		  return render_template('adduser.html', form=form)
	
 		      user = User(fname=form.fname.data, lname=form.lname.data, email=form.email.data)
 		      try:
 		          db.session.add(user)
 		          db.session.commit()
  		     except Exception:
  		         db.session.rollback()
 		     return render_template('adduser_confirmation.html', title = 'Add User Confirmation', username=form.fname.data)
 		  return render_template('adduser.html', title = 'User Input Form', form = form)

		@app.route("/users/")
		def display_users():
		    users = User.query.all()
		    fullnames = [ user.fname + ' ' + user.lname for user in users ]
		    return render_template('users.html', users=fullnames)



    main.py
    =======
        from helloapp import app
    forms.py  [for web form, The belowe defined form asks user to enter first name, last name and email address as inputs.]
    ========
	from flask_wtf import FlaskForm
	from wtforms import StringField, SubmitField, validators
	class UserForm(FlaskForm):
  	 fname = StringField("First Name",  [validators.DataRequired(message="[This field is required.]"), validators.Length(min=3, max=100, message="[Field must be between 3 and 100 characters long.]")])
  	 lname = StringField("Last Name", [validators.DataRequired(), validators.Length(min=3, max=100)])
  	 email = StringField("Email", [validators.DataRequired(), validators.Email("Please provide a valid email address.")])
  	 submit = SubmitField("Submit")
	
    config.py
    ========= 
		import os
		basedir = os.path.abspath(os.path.dirname(__file__))
		class Config(object):
		    SECRET_KEY = os.environ.get('SECRET_KEY') or 'your-secret-key'  [for web form]
 		    # [this specify database location,If DATABASE_URL value is not defined, then SQLALCHEMY_DATABASE_URI value is set to hello.db database located in sampleproject folder.]
		    SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or 'sqlite:///' + os.path.join(basedir, 'hello.db') 
		    # [is a setting which allows tracking of modifications done to database]
		    SQLALCHEMY_TRACK_MODIFICATIONS = False

    models.py   #for database
    =========
	from helloapp import db
	class User(db.Model):
    		id = db.Column(db.Integer, primary_key=True)
    		fname = db.Column(db.String(100), index=True)
    		lname = db.Column(db.String(100), index=True)
   		email = db.Column(db.String(120), index=True, unique=True)
   		
		def __repr__(self):
       			 return "<User : {}>".format(self.fname+' '+self.lname)		


After doing above from command line virenv: export FLASK_APP=main  [environment variable]
now start application by : flask run


connecting to database
======================
flask extension for connecting database: pip install flask-sqlalchemy
let's create a configuration file config.py inside helloapp for sqllite database

You can now create a migration repository using flask db init
cmd: flask db env
The command creates migrations folder with required files and folders in sampleproject.

create migration: flask db migrate -m "Creating user table"


The migration script created using : 'flask db migrate' command contains two function definitions: upgrade and downgrade.

To apply the changes to a database, you have to run the command

cmd: flask db upgrade

inserting data
--------------
flask shell
from helloapp.models import User
user1 = User(fname="James", lname="smith", email="james@abc.com")
user2 = User(fname="Sam", lname="Billings", email="sam@xyz.com")

commit
------
from helloapp import db
db.session.add(user1)
db.session.add(user2)
db.session.commit()

query
-----
User.query.all()
[<User : James smith>, <User : Sam Billings>]
User.query.filter(User.fname == 'James').all()
[<User : James smith>]


flask webforms
=============
pip install flask-wtf


DOCKER
=======
=======
docker is better than VM can be allocated dynamic size container.
its build on operating system not on physical resources(storage,ram,network,cpu) unlike Virtual machine.
if first VM(out of 4 which built on physical device) doesnot use anything that physical resource is unutilized by other 3 VMs.
docker makes isolation and reproducibility possible.

a simple flask model app
========================
from flask import Flask, request
from flasgger import Swagger   # this will give nice UI for api @ localhost:5000/apidocs then show hide that will show all operations
import pickle

app=Flask(__name)
swagger=Swagger(app)

with open('/user/rf.pickle','rb') as m:
model=pickle.load(m)

@app.route('/predict')
def predict_iris():
	l=request.args.get("length")
	s=request.args.get("width")
	prediction=model.predict(l,s)
	return str(prediction))

@app.route('/predict_file', method=['POST'])
def predict_iris_file():
	inputdata=pd.read_csv(request.files.get("input_file"))
	prediction=model.predict(inputdata)
	return str(prediction))
	
def cleanse_text(text):
    if text:
        #whitespaces
        clean = ' '.join(text.split())
        
        # Stemming
        red_text = [stem(word) for word in clean.split()]
        
        # Done. return
        return ' '.join(red_text)

    else:
        return text

#create some plot and downloadable files
@app.route('/cluster', methods=['POST'])
def cluster():
    data = pd.read_csv(request.files['dataset'])
    
    unstructure = 'text'
    if 'col' in request.args:
        unstructure = request.args.get('col')
    no_of_clusters = 2
    if 'no_of_clusters' in request.args:
        no_of_clusters = int(request.args.get('no_of_clusters'))
        
    data = data.fillna('NULL')
    
    data['clean_sum'] = data[unstructure].apply(cleanse_text)
    
    vectorizer = CountVectorizer(analyzer='word',
                                 stop_words='english')
    counts = vectorizer.fit_transform(data['clean_sum'])
    
    kmeans = KMeans(n_clusters=no_of_clusters)
    
    data['cluster_num'] = kmeans.fit_predict(counts)
    data = data.drop(['clean_sum'], axis=1)
    
    output = BytesIO()
    writer = pd.ExcelWriter(output, engine='xlsxwriter')
    data.to_excel(writer, sheet_name='Clusters', 
                  encoding='utf-8', index=False)
    
    clusters = []
    for i in range(np.shape(kmeans.cluster_centers_)[0]):
        data_cluster = pd.concat([pd.Series(vectorizer.get_feature_names()),
                                  pd.DataFrame(kmeans.cluster_centers_[i])],
        axis=1)
        data_cluster.columns = ['keywords', 'weights']
        data_cluster = data_cluster.sort_values(by=['weights'], ascending=False)
        data_clust = data_cluster.head(n=10)['keywords'].tolist()
        clusters.append(data_clust)
    pd.DataFrame(clusters).to_excel(writer, sheet_name='Top_Keywords', encoding='utf-8')
    
    #Pivot
    data_pivot = data.groupby(['cluster_num'], as_index=False).size()
    data_pivot.name = 'size'
    data_pivot = data_pivot.reset_index()
    data_pivot.to_excel(writer, sheet_name='Cluster_Report', 
                  encoding='utf-8', index=False)
    
    # insert chart
    workbook = writer.book
    worksheet = writer.sheets['Cluster_Report']
    chart = workbook.add_chart({'type': 'column'})
    chart.add_series({
            'values': '=Cluster_Report!$B$2:$B'+str(no_of_clusters+1)
            })
    worksheet.insert_chart('D2', chart)
    
    writer.save()
    
    memory_file = BytesIO()  # make file downloadable
    with zipfile.ZipFile(memory_file, 'w') as zf:
        names = ['cluster_output.xlsx']
        files = [output]
        for i in range(len(files)):
            data = zipfile.ZipInfo(names[i])
            data.date_time = time.localtime(time.time())[:6]
            data.compress_type = zipfile.ZIP_DEFLATED
            zf.writestr(data, files[i].getvalue())
    memory_file.seek(0)
    response = make_response(send_file(memory_file, attachment_filename='cluster_output.zip',
                                       as_attachment=True))
    response.headers['Access-Control-Allow-Origin'] = '*'
    
    return response	

model = load_model('./model.h5')

@app.route('/predict_digit', methods=['POST'])   #takes input image from file and predict
def predict_digit():
    """Example endpoint returning a prediction of mnist
    ---
    parameters:
        - name: image
          in: formData
          type: file
          required: true
    """
    im = Image.open(request.files['image'])
    im2arr = np.array(im).reshape((1, 1, 28, 28))
    return str(np.argmax(model.predict(im2arr)))
    
if __name__=='__main__':
	app.run(host='0.0.0.0',port=5000)


essential structure base image
==============================
linux-->anaconda-->flask,flasgger--> docker
using FROM command we can define

docker commands
===============
FROM: FROM continuumio/anaconda3:4.4.0 [anaconda3 for python3]
MAINTAINER UNP, https://unp.education
COPY: [copy to docker container.] COPY ./flask_demo[my local files] /usr/local/python/[docker directory]
EXPOSE: expose to world. EXPOSE 5000
WORKDIR: specify dir of container. /usr/local/python/
RUN: any command given here will run inside docker. pip install -r requirments.txt[all packages required]
RUN: python -m nltk.downloader average_tagger_perceptron
CMD: python flask-api.py. flask-api.py should be inside flask_demo folder, so is picklefile and requirments.txt   [this is for development server]

For each RUN docker will create new image so 2 RUN for 2 commands so we will sqeeze as below
RUN: pip install -r requirments.txt \
&& python -m nltk.downloader average_tagger_perceptron

save the above file as with no extension [dockerfile] inside the folder where flask_demo folder exists

docker file contains
---------------------
FROM continuumio/anaconda3:4.4.0 
MAINTAINER UNP, https://unp.education
COPY ./flask_demo /usr/local/python/
EXPOSE 5000
WORKDIR /usr/local/python/
RUN pip install -r requirments.txt \
&& python -m nltk.downloader average_tagger_perceptron
CMD python flask-api.py    

docker contains(from medium blog). Docker contanirize the code in one place to make independent
---------------------------------
FROM python:3.6-slim
COPY ./app.py /deploy/      --> copy from application dir ./app.py to docker curdur /deploy/ 
COPY ./requirements.txt /deploy/
COPY ./iris_trained_model.pkl /deploy/
WORKDIR /deploy/
RUN pip install -r requirements.txt
EXPOSE 80
ENTRYPOINT ["python", "app.py"]

0. Now keep  dockerfile and flask_demo in a place and navigate to that location. Under flask_demo  we have rf.pickle, flask-api.py and requirments.txt(flask,flasgger,mod_wsgi[for Apache]) file
1. install docker desktop and start. in window start docker terminal
2. navigate to folder where docker file exist
3. in docker terminal : docker build -t rf-api . [this will run all commands in docker file, and each command will give one image in terms of alphanumeric character]
4. Once images(like a plan) are done we can start multiple containers. 'docker images' command will show all images and there size
   command:docker run -p 5000:5000 rf-api --> this will run
5. now goto 0.0.0.0:5000/apidocs will show UI
6. docker ps --> will show running containers. docker stop 'container id' --> will stop

Now we will deploy in production webservers in Apache or so. Flask is not good for production
So we need WSGI(web server gateway interface) to connect Flask and Apache. This will connect Flask 'application' with Apache 'webserver'
When apache server is hit by user/machine request say 1000 req/sec, apache will route them to Flask via WSGI. Flask app will send response through WSGI.
Finally Apache push back response to user/machine.
So we will modify the last piece

To move to Apache
1. Include file flask-api.wsgi in same location as docker
flask-api.wsgi
----------
import sys
sys.path.insert(0,"/var/www/flask_predict_api")  [folder wher we place our script for apache to pickup www word is must]
sys.path.insert(0,'/opt/conda/lib/python3.6/site-packages') [python packages environment wsgi needs to know, libraries of python so that wsgi can build bridge]
sys.path.insert(0,'/opt/conda/bin')  [python packages environment wsgi needs to know, anaconda python installation]

import os
os.environ['PYTHONPATH']='/opt/conda/bin/python'  [pythonpath env variable is available so that apache knows which version of python to use]


from flask-api import app as application


2. change docker file
FROM continuumio/anaconda3:4.4.0 
MAINTAINER UNP, https://unp.education
EXPOSE 8000    [this will overwrite port in flask app]
RUN apt-get update && apt-get install -y apache2 \
    apache2-dev \
    vim \
 && apt-get clean \
 && apt-get autoremove \
 && rm -rf /var/lib/apt/lists/*
WORKDIR /var/www/flask_predict_api/  [root folder]
COPY ./flask-api.wsgi /var/www/flask_predict_api/flask_api.wsgi
COPY ./flask_demo /var/www/flask_predict_api/
RUN pip install -r requirments.txt 
RUN /opt/conda/bin/mod_wsgi-express install-module
RUN mod_wsgi-express setup-server flask_api.wsgi --port=8000 \  [setup server]
    --user www-data --group www-data \
    --server-root=/etc/mod_wsgi-express-80
CMD /etc/mod_wsgi-express-80/apachectl start -D FOREGROUND


3. build image and run: docker build -t apache-flask . [apache-flask is name of image]
4. run containers which leverages this image: docker run -d -p 8000:8000 apache-flask   [-d means start a deamon, p means local host port 8000 to bind to docker 8000 port, apache-flask is image name from above]
5. we can see actual containers: docker exec -it 'PORTS/container id' bash. We can do a ls to see list of files. TO see log vim /var/log/apache2/erro.log.
   if not found goto /etc/mod_wsgi-express-80 directory and then vim error_log, can see some log







